{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling using BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries/data required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20202007\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20202007\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20202007\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20202007\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import os\n",
    "from rake_nltk import Rake\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "import yake\n",
    "from ipywidgets import FloatProgress\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and perform preprocessing\n",
    "df = pd.read_csv(\"dataset_with_keywords.csv\", parse_dates=[\"date\"]) # Read data into 'df' dataframe\n",
    "docs = df[\"summary\"].tolist() # Create a list containing all article summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting BERTopic\n",
    "\n",
    "This might take a while on a CPU. In the background a pre-trained Large Language Model, called the sentence embedder, is used to convert the articles to a semantic vector space. We then perform clustering in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('southsudan_model'):\n",
    "    bertopic = BERTopic.load('southsudan_model')\n",
    "else:\n",
    "    #Due to the modularity of the model, there is a lot of randomness that hinders reproducibiity of the model.\n",
    "    #To fight this, you can for example set random state in the dimensionality reduction step via the following lines \n",
    "    #or explore a different approach\n",
    "\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, \n",
    "                 min_dist=0.0, metric='cosine', random_state=42)\n",
    "    bertopic = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model) # Initialize the BERTopic model\n",
    "    bertopic.fit_transform(docs) # Fit the model to the list of article summaries\n",
    "    bertopic.save(\"southsudan_model\") # Save the trained model as \"southsudan_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yake_keyword(dataframe):\n",
    "    \"\"\"Applies the yake library to a dataframe. Yake is a library that applies keyword extraction.\n",
    "    \n",
    "    Input: \n",
    "    - dataframe: A dataframe consisting out of a column with text that needs keyword extraction\n",
    "\n",
    "    Output:\n",
    "    - dataframe: Dataframe containing 2 extra columns (paragraph & summary) with the keywords determined by yake.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Implement a progress bar in the cell to show the progress.\n",
    "    tqdm.pandas()\n",
    "    # Apply the keyword extractor function from the NLP library yake.\n",
    "    language = 'en'\n",
    "    max_ngram_size = 2\n",
    "    deduplication_threshold = 0.9\n",
    "    numOfKeywords = 3  # <- Multiple keywords\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size,\n",
    "                                                dedupLim=deduplication_threshold,\n",
    "                                                top=numOfKeywords, features=None)\n",
    "\n",
    "    extractor = lambda x: custom_kw_extractor.extract_keywords(x)\n",
    "    # Apply the keyword extraction on both the summaries and the whole article content\n",
    "    df['paragraphs_3_keywords_2gram_summary'] = df['summary'].progress_apply(extractor)\n",
    "    df['keywords_paragraphs'] = df['paragraphs'].progress_apply(extractor)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:21\u001b[1;36m\u001b[0m\n\u001b[1;33m    tqdm.pandas(desc=\"Extracting Keywords from 'summary'\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def rake_extractor(text):\n",
    "    \"\"\" Determines the keywords.\"\"\"\n",
    "    r = Rake()\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    r.extract_keywords_from_text(text)\n",
    "    return list(r.get_word_degrees().keys())\n",
    "\n",
    "def rake_keywords(dataframe):\n",
    "        \"\"\"Applies the rake library to a dataframe. Rake is a library that applies keyword extraction.\n",
    "    \n",
    "    Input: \n",
    "    - dataframe: A dataframe consisting out of a column with text that needs keyword extraction\n",
    "\n",
    "    Output:\n",
    "    - dataframe: Dataframe containing 2 extra columns (paragraph & summary) with the keywords determined by rake.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Apply the extractor function with a progress bar to the 'summary' column\n",
    "    tqdm.pandas(desc=\"Extracting Keywords from 'summary'\")\n",
    "    dataframe['summary_rake_keywords'] = dataframe['summary'].progress_apply(rake_extractor)\n",
    "\n",
    "    # Apply the extractor function with a progress bar to the 'paragraphs' column\n",
    "    tqdm.pandas(desc=\"Extracting Keywords from 'paragraphs'\")\n",
    "    dataframe['paragraphs_rake_keywords'] = dataframe['paragraphs'].progress_apply(rake_extractor)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to calculate a list of the top n topics related to (a) given keyword(s)\n",
    "\n",
    "def get_relevant_topics(bertopic_model, keywords, top_n):\n",
    "    '''\n",
    "    Retrieve a list of the top n number of relevant topics to the provided (list of) keyword(s)\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        bertopic_model: a (fitted) BERTopic model object\n",
    "        \n",
    "        keywords:   a string containing one or multiple keywords to match against,\n",
    "                    \n",
    "                    This can also be a list in the form of ['keyword(s)', keyword(s), ...]\n",
    "                    \n",
    "                    In this case a maximum of top_n topics will be found per list element \n",
    "                    and subsetted to the top_n most relevant topics.\n",
    "                    \n",
    "                    !!!\n",
    "                    Take care that this method only considers the relevancy per inputted keyword(s) \n",
    "                    and not the relevancy to the combined list of keywords.\n",
    "                    \n",
    "                    In other words, topics that appear in the output might be significantly related to a \n",
    "                    particular element in the list of keywords but not so to any other element, \n",
    "                    \n",
    "                    while topics that do not appear in the output might be significantly related to the \n",
    "                    combined list of keywords but not much to any of the keyword(s) in particular.\n",
    "                    !!!\n",
    "                    \n",
    "        top_n: an integer indicating the number of desired relevant topics to be retrieved\n",
    "        \n",
    "        \n",
    "        Return: a list of the top_n (or less) topics most relevant to the (list of) provided keyword(s)\n",
    "    '''\n",
    "    \n",
    "    if type(keywords) is str: keywords = [keywords] # If a single string is provided convert it to list type\n",
    "    \n",
    "    relevant_topics = list() # Initilize an empty list of relevant topics\n",
    "    \n",
    "    for keyword in keywords: # Iterate through list of keywords\n",
    "        \n",
    "        # Find the top n number of topics related to the current keyword(s)\n",
    "        topics = bertopic_model.find_topics(keyword, top_n = top_n)\n",
    "        \n",
    "        # Add the topics to the list of relevant topics in the form of (topic_id, relevancy)\n",
    "        relevant_topics.extend(\n",
    "            zip(topics[0], topics[1]) # topics[0] = topic_id, topics[1] = relevancy\n",
    "        )\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1]) # Sort the list of topics on ASCENDING ORDER of relevancy\n",
    "    \n",
    "    # Get a list of the set of unique topics (with greates relevancy in case of duplicate topics)\n",
    "    relevant_topics = list(dict(relevant_topics).items())\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1], reverse=True) # Now sort the list of topics on DESCENDING ORDER of relevancy\n",
    "    \n",
    "    return relevant_topics[:10] # Return a list of the top_n unique relevant topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rake and yake to a dataframe and store it.\n",
    "\n",
    "# df = rake_keywords(df)\n",
    "# df = yake_keyword(df)\n",
    "# df.to_csv('dataset_with_keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run from here if dataset already contains keywords from yake and rake!\n",
    "## Determination of keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Rake keywords with the summarized articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article</td>\n",
       "      <td>18500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>discusses</td>\n",
       "      <td>18499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>south</td>\n",
       "      <td>16898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sudan</td>\n",
       "      <td>16534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>government</td>\n",
       "      <td>6593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16888</th>\n",
       "      <td>sympathize</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16889</th>\n",
       "      <td>predictions</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16891</th>\n",
       "      <td>destabilized</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16895</th>\n",
       "      <td>furthering</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25680</th>\n",
       "      <td>sigh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25681 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              index  values\n",
       "0           article   18500\n",
       "1         discusses   18499\n",
       "6             south   16898\n",
       "7             sudan   16534\n",
       "151      government    6593\n",
       "...             ...     ...\n",
       "16888    sympathize       1\n",
       "16889   predictions       1\n",
       "16891  destabilized       1\n",
       "16895    furthering       1\n",
       "25680          sigh       1\n",
       "\n",
       "[25681 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------- You can only run this line once after you initialized df as the dataframe. --------------\n",
    "df['summary_rake_keywords'] = df['summary_rake_keywords'].apply(ast.literal_eval)\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Iterate through the DataFrame and print each item in the lists\n",
    "keywords_list = []\n",
    "for index, row in df.iterrows():\n",
    "    for item in row['summary_rake_keywords']:\n",
    "        keywords_list.append(item)\n",
    "\n",
    "# Count the keywords and sort them using the Counter library\n",
    "counts = Counter(keywords_list)\n",
    "\n",
    "# Create a dataframe to sort the keywords more easily\n",
    "keywords_rake_summary = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "keywords_rake_summary.rename( columns={0 :'values'}, inplace=True )\n",
    "keywords_rake_summary.sort_values(by='values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top rake keywords with the paragraphs from the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sudan</td>\n",
       "      <td>17924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>south</td>\n",
       "      <td>17761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>said</td>\n",
       "      <td>13722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>government</td>\n",
       "      <td>12767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>people</td>\n",
       "      <td>12279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46939</th>\n",
       "      <td>faroqu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46937</th>\n",
       "      <td>martials</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46934</th>\n",
       "      <td>marbles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13776</th>\n",
       "      <td>esheraya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72134</th>\n",
       "      <td>paye</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72135 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index  values\n",
       "4           sudan   17924\n",
       "3           south   17761\n",
       "15           said   13722\n",
       "80     government   12767\n",
       "51         people   12279\n",
       "...           ...     ...\n",
       "46939      faroqu       1\n",
       "46937    martials       1\n",
       "46934     marbles       1\n",
       "13776    esheraya       1\n",
       "72134        paye       1\n",
       "\n",
       "[72135 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAKE paragraphs\n",
    "# Iterate through the DataFrame and print each item in the lists\n",
    "\n",
    "# --------------- You can only run this line once after you initialized df as the dataframe. --------------\n",
    "df['paragraphs_rake_keywords'] = df['paragraphs_rake_keywords'].apply(ast.literal_eval)\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "keywords_list = []\n",
    "for index, row in df.iterrows():\n",
    "    for item in row['paragraphs_rake_keywords']:\n",
    "        keywords_list.append(item)\n",
    "\n",
    "# Count the keywords and sort them using the Counter library\n",
    "counts = Counter(keywords_list)\n",
    "\n",
    "# Create a dataframe to sort the keywords more easily\n",
    "keywords_rake_paragraphs = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "keywords_rake_paragraphs.rename( columns={0 :'values'}, inplace=True )\n",
    "keywords_rake_paragraphs.sort_values(by='values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top yake keywords from the summary and paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns to the dataframe of the keyword determination with YAKE\n",
    "df_keywords_summary = pd.DataFrame(df['keywords_summary'])\n",
    "df_keywords_paragraph = pd.DataFrame(df['keywords_paragraphs'])\n",
    "\n",
    "# YAKE summary\n",
    "df_keywords_summary['Extracted_Words'] = df_keywords_summary['keywords_summary'].apply(lambda x: re.search(r'[A-Z][a-z]+', x).group() if re.search(r'[A-Z][a-z]+', x) else \"\")\n",
    "df_keywords_summary = pd.DataFrame.from_dict(df_keywords_summary[['Extracted_Words']].value_counts().to_dict(), orient='index').reset_index()\n",
    "df_keywords_summary.rename(columns={0 :'values'}, inplace=True )\n",
    "\n",
    "# YAKE paragraphs\n",
    "# Create a dataframe to sort the keywords more easily\n",
    "df_keywords_paragraph['Extracted_Words'] = df_keywords_paragraph['keywords_paragraphs'].apply(lambda x: re.search(r'[A-Z][a-z]+', x).group() if re.search(r'[A-Z][a-z]+', x) else \"\")\n",
    "df_keywords_paragraph = pd.DataFrame.from_dict(df_keywords_paragraph[['Extracted_Words']].value_counts().to_dict(), orient='index').reset_index()\n",
    "df_keywords_paragraph.rename(columns={0 :'values'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the the top provided by the cells above, 14 words are chosen for this project.\n",
    "The keywords are listed here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rake == generalish\n",
    "chosen_keywords_rake_summary = ['government', 'president', 'peace', 'juba', 'conflict', 'security',\n",
    "                            'violence', 'international', 'un', 'support', 'humanitarian', 'oil',\n",
    "                            'war', 'machar']\n",
    "                          \n",
    "chosen_keywords_rake_paragraphs = ['government', 'people','juba','president', 'state', 'peace','security',\n",
    "                                    'international','conflict', 'year', 'national', 'united', 'war', 'political']   \n",
    "\n",
    "# Yake == location\n",
    "chosen_keywords_yake_paragraphs = ['juba', 'abyei', 'president', 'machar', 'Uganda', 'jonglei', 'darfur',\n",
    "                            'nile', 'minister', 'nuer', 'khartoum', 'government', 'police', 'ethiopia',]\n",
    "\n",
    "chosen_keywords_yake_summary = ['president', 'jonglei','uganda', 'abyei', 'machar', 'united', 'bor', 'ethiopia'\n",
    "                                 ,'republic', 'ababa', 'bentiu', 'malakal', 'kenya', 'unity']                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keywords_as_columns_to_dataframe(chosen_words_list_all_possibilities, dataframe):\n",
    "    \"\"\"Add the keywords generated by key word extraction method as a column to a dataframe \n",
    "    with a value containing True or False depending on wheteher the keyword relates to the article.\n",
    "\n",
    "    Input:\n",
    "    - chosen_words_list_all_possibilities: list of keywords\n",
    "\n",
    "    Output:\n",
    "    - dataframe\n",
    "    \"\"\"\n",
    "    for item in tqdm(chosen_words_list_all_possibilities):\n",
    "        print(item)\n",
    "        # Get the top 10 topics related to the keywords 'hunger' and 'food insecurity'\n",
    "        relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=item, top_n=10)\n",
    "\n",
    "        topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "        # for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "        #     print(topic_id, relevancy)\n",
    "\n",
    "        item = str([item])   \n",
    "        dataframe[item] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "        # View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "        # bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From keywords to dataframe ready for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95a3463b9a34a49b819e5b5577c4873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government\n",
      "people\n",
      "juba\n",
      "president\n",
      "state\n",
      "peace\n",
      "security\n",
      "international\n",
      "conflict\n",
      "year\n",
      "national\n",
      "united\n",
      "war\n",
      "political\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e1b8ab01c54089b76c68b5b33208e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government\n",
      "president\n",
      "peace\n",
      "juba\n",
      "conflict\n",
      "security\n",
      "violence\n",
      "international\n",
      "un\n",
      "support\n",
      "humanitarian\n",
      "oil\n",
      "war\n",
      "machar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb29f581214c41a8bc248106cc8b9572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juba\n",
      "abyei\n",
      "president\n",
      "machar\n",
      "Uganda\n",
      "jonglei\n",
      "darfur\n",
      "nile\n",
      "minister\n",
      "nuer\n",
      "khartoum\n",
      "government\n",
      "police\n",
      "ethiopia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6da40dc22a48b19e3edceadb4d3f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president\n",
      "jonglei\n",
      "uganda\n",
      "abyei\n",
      "machar\n",
      "united\n",
      "bor\n",
      "ethiopia\n",
      "republic\n",
      "ababa\n",
      "bentiu\n",
      "malakal\n",
      "kenya\n",
      "unity\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the different dataframes\n",
    "df_rake_paragraphs = add_keywords_as_columns_to_dataframe(chosen_keywords_rake_paragraphs, df_rake_paragraphs)\n",
    "df_rake_summary = add_keywords_as_columns_to_dataframe(chosen_keywords_rake_summary, df_rake_summary)\n",
    "df_yake_paragraphs = add_keywords_as_columns_to_dataframe(chosen_keywords_yake_paragraphs, df_yake_paragraphs)\n",
    "df_yake_summary = add_keywords_as_columns_to_dataframe(chosen_keywords_yake_summary, df_yake_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.3', 'Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0', 'index',\n",
       "       'name', 'summary', 'date', 'lat', 'lng', 'paragraphs', 'publisher',\n",
       "       'tags', 'keywords_summary', 'keywords_paragraphs',\n",
       "       'paragraphs_3_keywords_2gram_summary', 'summary_rake_keywords',\n",
       "       'paragraphs_rake_keywords', '['government']', '['people']', '['juba']',\n",
       "       '['president']', '['state']', '['peace']', '['security']',\n",
       "       '['international']', '['conflict']', '['year']', '['national']',\n",
       "       '['united']', '['war']', '['political']'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rake_paragraphs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the csv's\n",
    "df_rake_paragraphs.to_csv('df_rake_paragraphs.csv')\n",
    "df_rake_summary.to_csv('df_rake_summary.csv')\n",
    "df_yake_paragraphs.to_csv('df_yake_paragraphs.csv')\n",
    "df_yake_summary.to_csv('df_yake_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add chatgpt generated keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The following Chatgpt interaction was used:</b>\n",
    "\n",
    "consider the situation of South Sudan. Present 20 singular keywords in a list format that describe the past situation of South Sudan.\n",
    "\n",
    "ChatGPT\n",
    "\n",
    "Certainly, here are 20 keywords that describe the past situation of South Sudan:\n",
    "\n",
    "~\n",
    "\n",
    "User\n",
    "\n",
    "The keywords may only consist of a single word. abbreviations of parties are allowed\n",
    "\n",
    "ChatGPT\n",
    "\n",
    "Understood, here are 20 single-word keywords and abbreviations that describe the past situation of South Sudan:\n",
    "\n",
    "~\n",
    "\n",
    "User\n",
    "\n",
    "Civil War is not a single word, reconsider the list\n",
    "\n",
    "ChatGPT\n",
    "\n",
    "Certainly, here's a revised list of single-word keywords and abbreviations that describe the past situation of South Sudan:\n",
    "\n",
    "~\n",
    "\n",
    "User\n",
    "\n",
    "can you put quotations around the words and a comma in between like a python list format?\n",
    "\n",
    "ChatGPT\n",
    "\n",
    "Certainly, here's the list of single-word keywords and abbreviations for the past situation of South Sudan in Python list format:\n",
    "\n",
    "~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_keywords_as_columns_to_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\20202007\\Documents\\GitHub\\JBG060-DC3-23-24-public\\topic_modelling.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gpt_keywords \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mIndependence\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSecession\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mConflict\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFamine\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPeace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRefugees\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEthnicity\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUNMISS\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSPLA\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mOil\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDisplacement\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mChildren\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mReferendum\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mInstability\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mClashes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDrought\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCorruption\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMediation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCrisis\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRebel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_gpt \u001b[39m=\u001b[39m add_keywords_as_columns_to_dataframe(gpt_keywords)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_gpt\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mdf_gpt.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'add_keywords_as_columns_to_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "gpt_keywords = [\"Independence\", \"Secession\", \"Conflict\", \"Famine\", \"Peace\", \"Refugees\", \"Ethnicity\", \"UNMISS\", \"SPLA\", \"Oil\", \"Displacement\", \"Children\", \"Referendum\", \"Instability\", \"Clashes\", \"Drought\", \"Corruption\", \"Mediation\", \"Crisis\", \"Rebel\"]\n",
    "df_gpt = add_keywords_as_columns_to_dataframe(gpt_keywords)\n",
    "df_gpt.to_csv('df_gpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
