{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling using BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries/data required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20202007\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20202007\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20202007\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20202007\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and perform preprocessing\n",
    "\n",
    "df = pd.read_csv(\"data/articles_summary_cleaned.csv\", parse_dates=[\"date\"]) # Read data into 'df' dataframe\n",
    "print(df.shape) # Print dataframe shape\n",
    "\n",
    "docs = df[\"summary\"].tolist() # Create a list containing all article summaries\n",
    "\n",
    "df.head() # Show first 5 dataframe entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting BERTopic\n",
    "\n",
    "This might take a while on a CPU. In the background a pre-trained Large Language Model, called the sentence embedder, is used to convert the articles to a semantic vector space. We then perform clustering in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('southsudan_model'):\n",
    "    bertopic = BERTopic.load('southsudan_model')\n",
    "else:\n",
    "    bertopic = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True) # Initialize the BERTopic model\n",
    "\n",
    "    bertopic.fit_transform(docs) # Fit the model to the list of article summaries\n",
    "    bertopic.save(\"southsudan_model\") # Save the trained model as \"southsudan_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to the modularity of the model, there is a lot of randomness that hinders reproducibiity of the model.\n",
    "#To fight this, you can for example set random state in the dimensionality reduction step via the following lines \n",
    "#or explore a different approach\n",
    "\n",
    "#from bertopic import BERTopic\n",
    "#from umap import UMAP\n",
    "\n",
    "#umap_model = UMAP(n_neighbors=15, n_components=5, \n",
    "#                  min_dist=0.0, metric='cosine', random_state=42)\n",
    "#topic_model = BERTopic(umap_model=umap_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive visualization of the vector space\n",
    "\n",
    "As you can see, documents with related topics are close in the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic.visualize_documents(docs) # Create a plot of the topics, this may take a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating smaller topics\n",
    "\n",
    "Within our list of topics, we find topics that are semantically closest to 4 keywords:\n",
    "\n",
    "\"Hunger\", \"Refugees\", \"Conflict\", and \"Humanitarian\".\n",
    "\n",
    "**Feel free to change this approach!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to calculate a list of the top n topics related to (a) given keyword(s)\n",
    "\n",
    "def get_relevant_topics(bertopic_model, keywords, top_n):\n",
    "    '''\n",
    "    Retrieve a list of the top n number of relevant topics to the provided (list of) keyword(s)\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        bertopic_model: a (fitted) BERTopic model object\n",
    "        \n",
    "        keywords:   a string containing one or multiple keywords to match against,\n",
    "                    \n",
    "                    This can also be a list in the form of ['keyword(s)', keyword(s), ...]\n",
    "                    \n",
    "                    In this case a maximum of top_n topics will be found per list element \n",
    "                    and subsetted to the top_n most relevant topics.\n",
    "                    \n",
    "                    !!!\n",
    "                    Take care that this method only considers the relevancy per inputted keyword(s) \n",
    "                    and not the relevancy to the combined list of keywords.\n",
    "                    \n",
    "                    In other words, topics that appear in the output might be significantly related to a \n",
    "                    particular element in the list of keywords but not so to any other element, \n",
    "                    \n",
    "                    while topics that do not appear in the output might be significantly related to the \n",
    "                    combined list of keywords but not much to any of the keyword(s) in particular.\n",
    "                    !!!\n",
    "                    \n",
    "        top_n: an integer indicating the number of desired relevant topics to be retrieved\n",
    "        \n",
    "        \n",
    "        Return: a list of the top_n (or less) topics most relevant to the (list of) provided keyword(s)\n",
    "    '''\n",
    "    \n",
    "    if type(keywords) is str: keywords = [keywords] # If a single string is provided convert it to list type\n",
    "    \n",
    "    relevant_topics = list() # Initilize an empty list of relevant topics\n",
    "    \n",
    "    for keyword in keywords: # Iterate through list of keywords\n",
    "        \n",
    "        # Find the top n number of topics related to the current keyword(s)\n",
    "        topics = bertopic_model.find_topics(keyword, top_n = top_n)\n",
    "        \n",
    "        # Add the topics to the list of relevant topics in the form of (topic_id, relevancy)\n",
    "        relevant_topics.extend(\n",
    "            zip(topics[0], topics[1]) # topics[0] = topic_id, topics[1] = relevancy\n",
    "        )\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1]) # Sort the list of topics on ASCENDING ORDER of relevancy\n",
    "    \n",
    "    # Get a list of the set of unique topics (with greates relevancy in case of duplicate topics)\n",
    "    relevant_topics = list(dict(relevant_topics).items())\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1], reverse=True) # Now sort the list of topics on DESCENDING ORDER of relevancy\n",
    "    \n",
    "    return relevant_topics[:10] # Return a list of the top_n unique relevant topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'hunger' and 'food insecurity'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=['hunger', 'food insecurity'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"hunger\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'refugees' and 'displaced'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=['refugees', 'displaced'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"refugees\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keyword 'humanitarian'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=['humanitarian'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"humanitarian\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'conflict', 'fighting', and 'murder'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=['conflict', 'fighting', 'murder'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"conflict\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"data/articles_summary_cleaned.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Combine article summaries with the newly created features\n",
    "df = original_df.merge(\n",
    "    df[[\"summary\", \"hunger\", \"refugees\", \"humanitarian\", \"conflict\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"summary\",\n",
    "    right_on=\"summary\",\n",
    ")\n",
    "\n",
    "df.to_csv(\"data/articles_topics.csv\", index=False) # Save DataFrame to articles_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df[(df[\"hunger\"]==False) & (df[\"refugees\"] == False) & (df[\"humanitarian\"] == False) & (df[\"conflict\"] == False)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords extraction with yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414a8c83b23d4e5799d851115481bb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import yake\n",
    "from ipywidgets import FloatProgress\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "df = pd.read_csv('dataset_with_keywords.csv')\n",
    "\n",
    "language = 'en'\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 3  # <- Multiple keywords\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size,\n",
    "                                            dedupLim=deduplication_threshold,\n",
    "                                            top=numOfKeywords, features=None)\n",
    "\n",
    "# extract_keywords = lambda x: [k[0] for k in custom_kw_extractor.extract_keywords(x)]\n",
    "\n",
    "\n",
    "extractor = lambda x: custom_kw_extractor.extract_keywords(x)\n",
    "\n",
    "df['paragraphs_3_keywords_2gram_summary'] = df['summary'].progress_apply(extractor)\n",
    "df['keywords_paragraphs'] = df['paragraphs'].progress_apply(extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset_with_keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword extraction with rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multi-rake\n",
      "  Using cached multi_rake-0.0.2-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: numpy>=1.14.4 in c:\\users\\20202007\\documents\\github\\jbg060-dc3-23-24-public\\.conda\\lib\\site-packages (from multi-rake) (1.24.4)\n",
      "Collecting pycld2>=0.41 (from multi-rake)\n",
      "  Using cached pycld2-0.41.tar.gz (41.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyrsistent>=0.14.2 (from multi-rake)\n",
      "  Using cached pyrsistent-0.19.3-cp311-cp311-win_amd64.whl (62 kB)\n",
      "Requirement already satisfied: regex>=2018.6.6 in c:\\users\\20202007\\documents\\github\\jbg060-dc3-23-24-public\\.conda\\lib\\site-packages (from multi-rake) (2023.8.8)\n",
      "Building wheels for collected packages: pycld2\n",
      "  Building wheel for pycld2 (setup.py): started\n",
      "  Building wheel for pycld2 (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pycld2\n",
      "Failed to build pycld2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping c:\\Users\\20202007\\Documents\\GitHub\\JBG060-DC3-23-24-public\\.conda\\Lib\\site-packages\\numpy-1.25.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\20202007\\Documents\\GitHub\\JBG060-DC3-23-24-public\\.conda\\Lib\\site-packages\\numpy-1.25.2.dist-info due to invalid metadata entry 'name'\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [25 lines of output]\n",
      "      c:\\Users\\20202007\\Documents\\GitHub\\JBG060-DC3-23-24-public\\.conda\\Lib\\site-packages\\setuptools\\config\\setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              The license_file parameter is deprecated, use license_files instead.\n",
      "      \n",
      "              By 2023-Oct-30, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        parsed = self.parsers.get(option_name, lambda x: x)(value)\n",
      "      running bdist_wheel\n",
      "      The [wheel] section is deprecated. Use [bdist_wheel] instead.\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\pycld2\n",
      "      copying pycld2\\__init__.py -> build\\lib.win-amd64-cpython-311\\pycld2\n",
      "      running build_ext\n",
      "      building 'pycld2._pycld2' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pycld2\n",
      "ERROR: Could not build wheels for pycld2, which is required to install pyproject.toml-based projects\n",
      "WARNING: Skipping c:\\Users\\20202007\\Documents\\GitHub\\JBG060-DC3-23-24-public\\.conda\\Lib\\site-packages\\numpy-1.25.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\20202007\\Documents\\GitHub\\JBG060-DC3-23-24-public\\.conda\\Lib\\site-packages\\numpy-1.25.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\20202007\\Documents\\GitHub\\JBG060-DC3-23-24-public\\.conda\\Lib\\site-packages\\numpy-1.25.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "%pip install multi-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multirake'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\20202007\\Documents\\GitHub\\JBG060-DC3-23-24-public\\topic_modelling.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# %pip install multi-rake\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmultirake\u001b[39;00m \u001b[39mimport\u001b[39;00m Rake\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rake \u001b[39m=\u001b[39m Rake(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m    min_chars\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m    max_words\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m    generated_stopwords_min_freq\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202007/Documents/GitHub/JBG060-DC3-23-24-public/topic_modelling.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'multirake'"
     ]
    }
   ],
   "source": [
    " # %pip install multi-rake\n",
    "from multi_rake import Rake\n",
    "\n",
    "rake = Rake(\n",
    "    min_chars=3,\n",
    "    max_words=3,\n",
    "    min_freq=1,\n",
    "    language_code=None,  # 'en'\n",
    "    stopwords=None,  # {'and', 'of'}\n",
    "    lang_detect_threshold=50,\n",
    "    max_words_unknown_lang=2,\n",
    "    generated_stopwords_percentile=80,\n",
    "    generated_stopwords_max_len=3,\n",
    "    generated_stopwords_min_freq=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[('Sudan', 0.006790420531629885)]\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_with_keywords.csv')\n",
    "df['keywords_paragraphs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of articles that do not get sorted into either of the categories. So, feel free to change or expand this approach!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
